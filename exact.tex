\documentclass{article}


\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{microtype}
\usepackage{lettrine}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage[bitstream-charter]{mathdesign}
\usepackage{XCharter}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{mathtools}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{external}
\usetikzlibrary{positioning, shapes, fit, 3d, perspective, shadings, intersections}

\input{math_commands}
\newcommand\layernorm{\mathrm{layernorm}}
\newcommand\standardize{\mathrm{standardize}}
\newcommand\logits{\mathrm{logits}}
\newcommand\diag{\mathrm{diag}}
\newcommand\linalgspan{\mathrm{span}}

\author{Matthew Finlayson}
\title{Taking API-Protected Language Model Attacks One Layer Deeper}
\date{}

\begin{document}
\maketitle

\begin{abstract}
  To protect trade secrets, 
  language model (LM) providers often limit access to their models
  by exposing them only via a restrictive API.
  Recent work showed that under certain common configurations 
  these APIs leak non-public information 
  about their underlying LM architectures,
  such as the model's embedding size.
  The core observation that makes these attacks possible
  is that the final layer of the LM imposes linear constraints
  on the model outputs.
  However, the attack methods proposed thus far reveal only limited information 
  about the final layer.
  For instance, they reveal the LM's output space,
  but not the actual parameters of the output layer.
  This paper introduces a method 
  for exposing additional information about the LM output layer,
  in particular finding the singular values (scaling factors),
  rotation, and bias term of the model's final affine transformation.
  This is accomplished by observing the constraints 
  imposed by the penultimate layer of the model, i.e., the layernorm.
  In particular, the layernorm constrains the model outputs to an ellipsoid.
  The additional information gained from this attack unlocks several capabilities,
  including recovering correctly-scaled model activations,
  bounding the magnitude of the model parameters,
  bounding the probability that the model can put on specific tokens,
  and more accurately anticipating LM sampling errors.
  Each of these capabilities in turn have many potential downstream use cases.
  Thus this one-layer-deeper API LM attack 
  constitutes an significant result in the field of LM API security.
\end{abstract}

\section{Finding the output layer scale, rotation, and bias}

\begin{figure}
  \centering
  \small
  \input{fig/arch}
  \caption{
    A typical language model uses a layernorm layer 
    followed by a linear projection to obtain logits.
    Equivalently, we view these operations as standardization~(\ref{eq:standardize}) 
    followed by an affine transformation~\(\vx\mapsto\mW\diag(\gamma)\vx+\mW\beta\). 
    We further decompose the affine transformation 
    via singular value decomposition (SVD) 
    into pure rotation and scaling 
    operations~\(\mU\Sigma\mV^\top=\mW\diag(\gamma)\)
    and a bias term~\(\vb=\mW\beta\).
    We show that it is possible to recover the parameters~\(\Sigma\), \(\vb\),
    and the magnitudes and directions (but not signs) of the columns of \(\mU\)
    by observing logit outputs from the model.
    % TODO: rows or columns?
  }
  \label{fig:arch}
\end{figure}

\lettrine{I}{n previous work,} \citet{Finlayson2024LogitsOA} and \citet{Carlini2024StealingPO} showed that it is possible to recover information about an LM's final layer by observing the structure of the LM outputs.
In particular, they observe that LM outputs are restricted to a low-dimensional space, and are able to discover this space by observing the outputs.
This is accomplished by finding a matrix~\(\mL\) whose columns span the model's output space, i.e., all outputs from the model are a linear combination of the columns of \(\mL\).
Equivalently, this means that \(\mL\) is a \emph{linear transformation} of the LM's output embedding matrix~\(\mW\),
i.e., \(\mW\mH=\mL\) for some unknown linear transformation~\(\mH\in\mathbb{R}^{d\times d}\).
\citeauthor{Finlayson2024LogitsOA} call this output space spanned by the columns of both \(\mW\) and \(\mL\) the \textit{model image}. 

The model image, i.e., \(\linalgspan(\mL)\),
gives some limited information about the model's actual parameters,
but it doesn't tell us how the embedding matrix~\(\mW\) 
rotates or scales the activations of the model.
It turns out that we can recover this additional information by leveraging the mathematical properties of the model's penultimate layer: the \textit{layer norm}.

\subsection{Decomposing output layer operations}

In order to isolate the rotations and scales in the model's output layer 
that we wish to recover,
we reformulate the model's output layers into an equivalent parameterization
that exposes these operations.
Figure~\ref{fig:arch} illustrates our reparametrization of 
\[\vx\mapsto\mW\layernorm_{\gamma,\beta}(\vx) \quad
\text{as} \quad \vx\mapsto\mU\Sigma\mV^\top\standardize(\vx)+\vb\]
where \(\mU\) and \(\mV^\top\) are unitary (i.e., rotation) matrices, 
\(\Sigma\) is a diagonal (i.e., scaling) matrix, and \(\vb\in\R^v\) is a bias term.
For the sake of clarity, we next explain this reparameterizatoin step by step.

The final two output layers of an LM are a layernorm
\[\layernorm(\vx)=\frac{\vx-\E[\vx]}{\sqrt{\Var(\vx)}}\gamma + \beta,\]
followed by a multiplication by the embedding matrix~\(\mW\). 
In other words, given an embedding~\(\vx\)
the model logits are \(\logits(\vx)=\mW\layernorm(\vx).\)
However, the logits can be equivalently viewed as a standardization
\begin{equation}
  \standardize(\vx)=\frac{\vx-\E[\vx]}{\sqrt{\Var(\vx)}}
  \label{eq:standardize}
\end{equation}
followed by an affine transformation, i.e.,
\[\logits(\vx)=\mW\diag(\gamma)\standardize(\vx) + \mW\beta.\]
Finally, using singular value decomposition (SVD),
there is some rotation~\(\mU\in\R^{v\times v}\),
scale~\(\Sigma\in\R^{v\times d}\) 
(a diagonal matrix with positive entries in descending order, known as the \emph{singular values}),
and second rotation~\(\mV\in\R^{d\times d}\)
such that \(\mU\Sigma\mV^\top=\mW\diag(\gamma)\).
This decomposition is \emph{almost} unique,
up to the ordering of equal scaling factors and their associated rotations.
For the purposes of this paper, consider it to be unique.
Secondly, we can re-parameterize \(\mW\beta\) as \(\vb\)
Therefore we arrive at 
\[\logits(\vx)=\mU\Sigma\mV^\top\standardize(\vx)+\vb.\]
This reformulation is an unique decomposition of the model's final layer.

\begin{figure}
  \centering
  \small
  \input{fig/standardize}
  \caption{The standardize function's output is restrited to the intersection of a plane and a sphere (indicated with a thick ring).}
  \label{fig:standardize}
\end{figure}

\subsection{Language model outputs are on hyperellipsoids}

I will next demonstrate how we can recover the parameters of \(\Sigma\), 
\(\vb=\mW\beta\), and the directions of the entries of \(\mU\)
by using the properties of the standardize function.
The co-domain of the standardize function
is the set of vectors with mean 0 and variance 1,
or in other words the vectors~\(\vx\) such that 
\[\sum_i^dx_i=0\quad\text{and}\quad \sum_i^dx_i^2=d.\]
This space corresponds to the intersection of a hypersphere of radius~\(\sqrt{d}\)
and the hyperplane perpendicular to the all-ones vector.
This intersection forms a (\(d-1\))-dimensional hypersphere,
as demonstrated in Figure~\ref{fig:standardize}.

\begin{figure}
  \centering
  \input{fig/affine}
  \caption{
    An ellipsoid is a sphere~(1) that has undergone an affine transformaton, in other words, a scale~(2), rotation~(3), and translation~(4).
    Since the outputs of the standardize function lie on a sphere
    and undergo an affine transformation to obtain the logits,
    we can recover the bias~\(\vb\) and scaling terms~\(\boldsymbol\sigma\)
    of an LM's output layer affine transformation
    by fitting an ellipse to the outputs of a language model 
    and observing what offset and scaling were applied to obtain it.
  }
\end{figure}

Now let us consider what happens to this hypersphere under the model's final set of transformations.
In particular, the points~\(\vx\) on the hypersphere are mapped to 
\(\mU\Sigma\mV^T\vx + \vb,\)
which as an \emph{affine transformation}.
This means that the co-domain of the model is an affine transformation of the hypersphere, i.e., an \emph{ellipsoid}.
Note that since the first operation \(\mV^T\) rotates the points on a hypersphere,
the outputs of this operation will still be on the hypersphere.
We can therefore remove this operation without changing the co-domain, 
and focus only on \(\mU\Sigma\vx + \vb.\)
Note that this means that \(\mV^\top\) is not recoverable via our method.

\subsection{Using model ellipses to steal model parameters}

Having established that model outputs lie on an ellipsoid, we now present an algorithm for obtaining model parameters~\(\Sigma,\mU,\vb\) from model outputs.

\begin{algorithm}
  \caption{Get output layer parameters of a language model.}
\begin{algorithmic}
  \Function{Method}{\(d\), \texttt{model}}
  \State \(m:=\binom{d+1}{2}+d\)
  \Comment Sample complexity for a \(d\)-ellipsoid (\S\ref{sec:samp})
  \State \(\vell_1,\vell_2,...,\vell_m\in\R^d\sim\texttt{model}\)
  \Comment Sample \(m\) outputs for tokens 1--\(d\)
  \State \(\mA,\vb = \textsc{EllipsoidFit}(\vell_1,\vell_2,\ldots,\vell_m)\)
  \Comment \citep{Lin2016FastME}, \(O(d^6)\)
  \State \(\mU', \Sigma, \_ = \textsc{SVD}(\mA)\)
  \Comment Singular value decomposition
  \State \Return \(\Sigma, \mU', \vb\)
  \EndFunction
\end{algorithmic}
\end{algorithm}

\section{The polynomial infeasibility of ellipsoid discovery}

\subsection{The sample cost of model ellipse discovery is super-cubic}
\label{sec:samp}

A hyperellipsoid (or simply ellipsoid) is a special case of a quadric hypersurface\footnotemark{} (or simply quadric).
\footnotetext{\url{https://en.wikipedia.org/wiki/Quadric}}
The general equation for a quadric with dimension~\(d\) has the form 
\[\sum_{i=1}^d\sum_{j=i}^dQ_{i,j}x_ix_j + \sum_{i=1}^dP_ix_i=1,\]
where \(Q\) and \(P\) are parameters.
The set of vectors \(\vx\in\R^d\) that satisfy this equation form the ellipsoid surface.
The total number of terms in the above equation is \(m=\binom{d+1}{2}+d=(d^2+3d)/2\).
Since the equation for a quadric is linear in its parameters,
a set of \(m\) points uniquely defines a quadric.

This method for solving for a \(d\)-dimensional hyperellipsoid means that for a model with a hidden size of 512, or \(2^9\), 
we would need \(2^{17} + 2^9 + 2^8=\num{131 840}\) samples. 
More generally, the number of samples is \(O(d^2)\).
This quadratic growth makes finding a model's ellipse from its outputs much more expensive than finding its image, which only requires \(O(d)\) samples.

In order to minimize cost, an attacker typically sends a single prefix token to an LLM API for each sample.
However, as the required number of samples surpasses the vocabulary size of the model it becomes necessary to send multi-token prefixes to the model in order to expand the number of unique prefixes.
The number of tokens per sample grows logarithmically with the number of samples required~\(m\), i.e., it grows at a rate of \(O(\log m)=O(\log d)\). 
Additionally, if the API only reveals a small number of logits per query we need \(O(d)\) queries per sample.
In all, this means that the cost of discovering the model ellipse grows at a rate of~\(O(d^3\log d)\).

Since the cost grows super-cubically with the hidden size, 
current API pricing makes it prohibitively expensive to obtain the ellipse of many popular LLMs, as shown in Table~\ref{tab:price}.
Though OpenAI's cheapest and smallest available generative model, \texttt{babbage-002}, is only about \$\num{1000} to attack, \texttt{gpt-3.5-turbo} costs over \$\num{150 000}.
However, if historic trends continue, these costs could drop significantly in the future.

\begin{table}
  \centering
  \small
  \begin{threeparttable}
  \input{tab/models.tex}
    \begin{tablenotes}
    \item[a] Confirmed size from \citet{Carlini2024StealingPO}.
    \item[b] Estimated size upper limit from \citet{Finlayson2024LogitsOA}.
    \end{tablenotes}
  \end{threeparttable}
  \caption{
    A summary of models, their sizes, the number of samples required to ascertain their output ellipsoid, and the cost of obtaining the samples, based on OpenAI inference pricing on June 7, 2024. The number of samples required grows quadratically with the embedding size, and the price per sample grows logarithmically with the number of samples.
  }
  \label{tab:price}
\end{table}

\subsection{Ellipsoid fitting takes sextic time}

Obtaining sufficient samples from an API-protected LLM is only the first step in finding the model ellipse. 
The second step is to fit an ellipse to the samples.
This, it turns out, is also difficult. 
There has been progress over recent decades on fast algorithms for multi-dimensional ellipsoid fitting~\citep{Calafiore2002ApproximationON, Ying2012AFA}, however the best known method still requires \(O(d^6)\) time~\citep{Lin2016FastME} which is prohibitively polynomial, as illustrated in Figure~\ref{fig:eigh}. 
Extrapolating our best-fit polynomial, finding the ellipse of a \texttt{gpt-3.5-turbo}-size model would take a laptop thousands of years.
If found, faster ellipsoid fitting methods could greadly reduce this figure,
but it could also be the case that current methods are already near the lower bound for ellipsoid fitting complexity.

We use the slower semi-definite programming algorithm from \citet{Ying2012AFA} in our experiments since it is easy to implement and numerically stable.

\begin{figure}
  \centering
  \small
  \begin{tikzpicture}
    \begin{groupplot}[
        group style={
          group size=2 by 1,
        },
        width=0.55\textwidth,
        height=5cm,
        xlabel={Dimension \(d\)},
      ]
      \nextgroupplot[
        legend style={legend pos=north west},
        ylabel={Time (minutes)},
        scaled y ticks=manual:{}{\pgfmathparse{#1/60}},
        ytick={0,{10 * 60}, {20 * 60}, {30 * 60}}
      ]
      \addplot[only marks] table {data/times.dat};
      \addlegendentry{\(\texttt{eigh}(\mH)\), \(\mH=\mH^\top\)}
      \addplot[raw gnuplot] gnuplot {
        f(x) = a * x ** 6 + b * x ** 5 + c * x ** 4 + d * x ** 3 + e * x ** 2 + f * x + g;
        a = 1; b = 0; c = 0; d = 0; e = 0; f = 0; g = 0;
        fit f(x) 'data/times.dat' using 1:2 via a,b,c,d,e,f,g;
        plot[0:210] f(x);
      };
      \addlegendentry{Best fit, \(O(6)\)}
      \nextgroupplot[
        width=0.45\textwidth,
        title=Extrapolation,
        scaled y ticks=manual:{}{\pgfmathparse{#1/60/525960/1000}},
        ytick={0,{3000 * 60 * 525960},{6000 * 60 * 525972}},
        ylabel={Time (millenia)},
      ]
      \addplot[only marks] table {data/times.dat};
      \addplot[raw gnuplot] gnuplot {
        f(x) = a * x ** 6 + b * x ** 5 + c * x ** 4 + d * x ** 3 + e * x ** 2 + f * x + g;
        a = 1; b = 0; c = 0; d = 0; e = 0; f = 0; g = 0;
        fit f(x) 'data/times.dat' using 1:2 via a,b,c,d,e,f,g;
        plot[0:4500] f(x);
      };
    \end{groupplot}
  \end{tikzpicture}
  \caption{
    Pictured here are the runtimes for taking the eigendecomposition of the symmetric matrix 
    composed of the quadratic terms of \(m\) \(d\)-dimensional points on an ellipsoid.
    Times are obtained on an M1 MacBook Pro
    using the \texttt{\_syevd} routine from LAPACK~\citep{laug} 
    via NumPy's \texttt{linalg.eigh} function~\citep{harris2020array}.
    This is the main computational bottleneck of the fastest known algorithm 
    for fitting a multi-dimensional elliposid~\citep{Lin2016FastME}, which takes \(O(d^6)\) time. 
    While still polynomial, this is a severe computational hurdle
    when LLMs have embedding dimensions in the thousands.
  }
  \label{fig:eigh}
\end{figure}

\section{Model ellipses as weak signatures}

\lettrine{T}{he authors of \citet{Finlayson2024LogitsOA}} identify several useful applications for the model image. 
Among them is the idea of using the model's unique output space (its ``image'') 
as a type of \textit{signature} to uniquely identify model outputs.
One shortcoming of their proposal 
is that anyone can cheaply obtain and share the model image,
meaning that any model can be retrofitted to share the target LM's image. 
This, combined with the fact that the original model be also be easily modified to change its image, (e.g., by continued training), means that the image is not very useful as a signature in an adversarial setting.
The prohibitive cost of discovering the model ellipse, however, makes it more suitable for at least one particular application, which we will illustrate.

Suppose Alice has a proprietary LM, protected by an API.
As an act of revenge for his recent termination,
Alice's former employee Bob decides to use not-yet revoked access to the company servers to download the model.
Bob then goes to Candice and tries to sell his stolen model to her.
Candice, knowing Bob's unscrupulous history, but wanting cheap access to the LM,
wishes to verify that Bob's model is indeed the one he says it is.
Candice can do this by simply checking that every output from Alice's model resides on Bob's model's ellipse. 
This can even be done without requiring Bob to reveal his model's weights.
If Candice obtains a random output from Alice's model, she can reveal any \(d-3\) elements from this output to Bob. Once this is done, Bob can give Candice two candidate values for each remaining element in the output, one of which will be correct.

\begin{figure}
  \centering
  \small
  \begin{tikzpicture}[node distance=2cm]
    \node (A) {Alice};
    \node (B) [right=of A] {Bob};
    \draw[->] (A) -- node[above] {1. Steals LM} (B);
    \node (C) [right=of B] {Candice};
    \draw[->] (A) to[bend right] node [below] {2. Gets random \(\vell\)} (C);
    \draw[->] (C) to[bend right] node [above] {3. Reveals \(\vell_{:d-3}\)} (B);
    \draw[->] (B) to node [below] {4. Sends \(\vell_{d-2}\)} (C);
  \end{tikzpicture}
  \hfill
  \begin{tikzpicture}[
      steplabel/.style={draw, fill=white},
      transformlabel/.style={fill=white, rounded corners, inner sep=1pt}
    ]
    \newcommand\yradius{0.7}
    \coordinate (input) at (0,0);
    \draw[name path=ellipse] (input) circle [x radius=1.5, y radius=\yradius, rotate=30];
    \draw[<->, name path=line] (0.5,1.2) -- (0.5,-1) node [below] {1. Candice reveals \(\vell_0\)};
    \fill [name intersections={of=ellipse and line}] 
    (intersection-1) circle (1.5pt) 
    (intersection-2) circle (1.5pt) node [right] {\(\vell\)};
    \node[fill=white] (B) at (-1, -0.25) {2. Bob guesses \(\vell\)};
    \draw[->, shorten >=2pt] (B) -- (intersection-1);
    \draw[->, shorten >=2pt] (B) -- (intersection-2);
  \end{tikzpicture}
\end{figure}



\bibliographystyle{chicago-annote}
\bibliography{refs}

\end{document}
