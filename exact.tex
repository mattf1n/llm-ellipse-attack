\documentclass{article}


\usepackage{microtype}
\usepackage{lettrine}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage[bitstream-charter]{mathdesign}
\usepackage{XCharter}
\usepackage[T1]{fontenc}
\usepackage{natbib}
\usepackage{mathtools}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{external}
\usetikzlibrary{positioning, shapes, fit, 3d, perspective, shadings, intersections}

\input{math_commands}
\newcommand\layernorm{\mathrm{layernorm}}
\newcommand\standardize{\mathrm{standardize}}
\newcommand\logits{\mathrm{logits}}
\newcommand\diag{\mathrm{diag}}

\author{Matthew Finlayson}
\title{Taking API-Protected Language Model Attacks One Layer Deeper}
\date{}

\begin{document}
\maketitle

\begin{abstract}
  To protect trade secrets, 
  language model (LM) providers often limit access to their models
  by exposing them only via a restrictive API.
  Recent work showed that under certain common configurations 
  these APIs leak non-public information 
  about their underlying LM architectures,
  such as the model's embedding size.
  The core observation that makes these attacks possible
  is that the final layer of the LM imposes linear constraints
  on the model outputs.
  However, the attack methods proposed thus far reveal only limited information 
  about the final layer.
  For instance, they reveal the LM's output space,
  but not the actual parameters of the output layer.
  This paper introduces a method 
  for exposing additional information about the LM output layer,
  in particular finding the singular values (scaling factors),
  rotation, and bias term of the model's final affine transformation.
  This is accomplished by observing the constraints 
  imposed by the penultimate layer of the model, i.e., the layernorm.
  In particular, the layernorm constrains the model outputs to an ellipsoid.
  The additional information gained from this attack unlocks several capabilities,
  including recovering correctly-scaled model activations,
  bounding the magnitude of the model parameters,
  bounding the probability that the model can put on specific tokens,
  and more accurately anticipating LM sampling errors.
  Each of these capabilities in turn have many potential downstream use cases.
  Thus this one-layer-deeper API LM attack 
  constitutes an significant result in the field of LM API security.
\end{abstract}

\section{Finding the scale and bias of the final layer}

\begin{figure}
  \centering
  \small
  \input{fig/arch}
  \caption{
    A typical language model uses a layernorm layer followed by a linear projection to obtain logits.
    Equivalently, these operations can be viewed as standardization followed by an affine transformation which can be uniquely decomposed further into rotations and scaling operations.
  }
\end{figure}

\lettrine{I}{n previous work,} \citet{Finlayson2024LogitsOA} and \citet{Carlini2024StealingPO} showed that it is possible to recover information about an LM's final layer by observing the structure of the LM outputs.
In particular, they observe that LM outputs are restricted to a low-dimensional space, and are able to discover this space by observing the outputs.
This is accomplished by finding a matrix~\(\hat\mW\) whose columns span the model's output space, i.e., all outputs from the model are a linear combination of the columns of \(\hat\mW\).
Equivalently, this means that \(\hat\mW\) is a \emph{linear transformation} of the LM's output embedding matrix~\(\mW\), i.e., \(\mW=\hat\mW\mA\) for some unknown linear transformation~\(\mA\in\mathbb{R}^d\).
\citeauthor{Finlayson2024LogitsOA} call this output space spanned by the columns of both \(\mW\) and \(\hat\mW\) the \textit{model image}. 

Though useful, the model image gives limited information about the model's actual parameters.
We know that the columns of the embedding matrix are in the span of the columns of \(\mW'\),
but we do not know how they rotate or scale the activations of the model.
However, it turns out that we can recover this additional information by leveraging the mathematical properties of the model's penultimate layer: the \textit{layer norm}.

Toward our goal of extracting information about the LM's internals,
we will begin by deriving an equivalent reformulation 
of a typical LM's final set of operations.
The purpose of this reformulation is to separate out components 
that we will be able to recover from those that we will not.

The final two output layers of an LM are a layernorm
\[\layernorm(\vx)=\frac{\vx-\E[\vx]}{\sqrt{\Var(\vx)}}\gamma + \beta,\]
followed by a multiplication by the embedding matrix~\(\mW\). 
In other words, given an embedding~\(\vx\)
the model logits are \(\logits(\vx)=\mW\layernorm(\vx).\)
However, the logits can be equivalently viewed as a standardization
\[\standardize(\vx)=\frac{\vx-\E[\vx]}{\sqrt{\Var(\vx)}}\]
followed by an affine transformation, i.e.,
\[\logits(\vx)=\mW\diag(\gamma)\standardize(\vx) + \mW\beta.\]
Finally, using singular value decomposition (SVD),
there is some rotation~\(\mU\in\R^{v\times v}\),
scale~\(\Sigma\in\R^{v\times d}\) 
(a diagonal matrix with positive entries in descending order, known as the \emph{singular values}),
and second rotation~\(\mV\in\R^{d\times d}\)
such that \(\mU\Sigma\mV^\top=\mW\diag(\gamma)\).
This decomposition is \emph{almost} unique,
up to the ordering of equal scaling factors and their associated rotations.
For the purposes of this paper, consider it to be unique.
Secondly, we can re-parameterize \(\mW\beta\) as \(\vb\)
Therefore we arrive at 
\[\logits(\vx)=\mU\Sigma\mV^\top\standardize(\vx)+\vb.\]
This reformulation is an unique decomposition of the model's final layer.

\begin{figure}
  \centering
  \small
  \input{fig/standardize}
  \caption{The standardize function's output is restrited to the intersection of a plane and a sphere (indicated with a thick ring).}
  \label{fig:standardize}
\end{figure}

I will next demonstrate how we can recover the parameters of \(\Sigma\), 
\(\vb=\mW\beta\), and the directions of the entries of \(\mU\)
by using the properties of the standardize function.
The co-domain of the standardize function
is the set of vectors with mean 0 and variance 1,
or in other words the vectors~\(\vx\) such that 
\[\sum_i^dx_i=0\quad\text{and}\quad \sum_i^dx_i^2=d.\]
This space corresponds to the intersection of a hypersphere of radius~\(\sqrt{d}\)
and the hyperplane perpendicular to the all-ones vector.
This intersection forms a (\(d-1\))-dimensional hypersphere,
as demonstrated in Figure~\ref{fig:standardize}.

\begin{figure}
  \centering
  \input{fig/affine}
  \caption{
    An ellipsoid is a sphere~(1) that has undergone an affine transformaton, in other words, a scale~(2), rotation~(3), and translation~(4).
    Since the outputs of the standardize function lie on a sphere
    and undergo an affine transformation to obtain the logits,
    we can recover the bias~\(\vb\) and scaling terms~\(\boldsymbol\sigma\)
    of an LM's output layer affine transformation
    by fitting an ellipse to the outputs of a language model 
    and observing what offset and scaling were applied to obtain it.
  }
\end{figure}

Now let us consider what happens to this hypersphere under the model's final set of transformations.
In particular, the points~\(\vx\) on the hypersphere are mapped to 
\(\mU\Sigma\mV^T\vx + \vb,\)
which as an \emph{affine transformation}.
This means that the co-domain of the model is an affine transformation of the hypersphere, i.e., an \emph{ellipsoid}.
Note that since the first operation \(\mV^T\) rotates the points on a hypersphere,
the outputs of this operation will still be on the hypersphere.
We can therefore remove this operation without changing the co-domain, 
so we will only conisder \(\mU\Sigma\vx + \vb.\)

Now that we know that the model outputs will be constrained to an ellipsoid,
we can find this ellipsoid by gathering a sufficient number of outputs 
and using least squares regression to fit an ellipsoid~\cite{Bertoni2010PreprintSM}.
The fitting algorithm immediately allows us to obtain \(\vb\), the center of the ellipsoid,
as well as \(\mC\) such that \(\mC=(\mA\mA^\top)^{-1}\) and \(\mA=\mU\Sigma\).
Unfortunately, we cannot directly obtain \(\mU\) and \(\Sigma\) from \(\mC\)
since there are multiple solutions \(\mA\) that satisfy the relation.
Employing Cholesky decomposition, 
we can obtain one such solution~\(\hat\mA\).
Running SVD on this solution and throwing away the initial rotation 
we obtain \(\hat\mA=\hat\mU\Sigma\).
This \(\Sigma\) is exactly the set of singular values we are looking for.
It also turns out that the columns of \(\hat\mU\) are either identical or negated columns of \(\mU\).
We therefore have the magnitude and directions of the rotation matrix entries, 
but not necessarily the correct signs.
This is due to the bilateral symmetries of the ellipsoid;
it is not possible tell whether the ellipsoid was rotated \(\theta\) radians in one direction
or \(\pi+\theta\) in the other.

Thus we have found the singular values of the output layer's affine transformation,
the translation parameters, and the directions of the rotation matrices.
Furthermore, we now have a more restricted set (the ellipsoid) from which the model outputs must come. 

\section{How many points define a hyperellipsoid?}
\lettrine{A}{ hyperellipsoid} (or simply ellipsoid) is a special case of a quadric hypersurface\footnotemark{} (or simply quadric).
\footnotetext{\url{https://en.wikipedia.org/wiki/Quadric}}
The general equation for a quadric with dimension~\(d\) has the form 
\[\sum_{i=1}^d\sum_{j=i}^dQ_{i,j}x_ix_j + \sum_{i=1}^dP_ix_i=1,\]
where \(Q\) and \(P\) are the parameters of the ellipsoid.
The set of vectors \(\vx\in\R^d\) that satisfy this equation form the ellipsoid surface.
The total number of terms in the above equation is \(n=(d^2+3d)/2\).
To solve for the parameters of an ellipsoid 
it suffices to collect \(n\) samples~\(
\mX=\begin{bmatrix}
\vx^1&\vx^2&\cdots&\vx^n
\end{bmatrix}\in\R^{d\times n}
\) from the ellipsoid surface,
construct the quadratic terms 
\[
  \prescript{\otimes}{}\vx^k = \begin{bmatrix}
  x^k_1x^k_1 & x^k_1x^k_2 & \cdots & x^k_1x^k_d & x^k_2x^k_2 & x^k_2x^k_3 & \cdots & x^k_dx^k_d & \vx^k
\end{bmatrix}\in\R^n
\] for each sample \(\vx^k\), 
from which we construct \(\prescript\otimes{}\mX=\begin{bmatrix}
  \prescript\otimes{}\vx^1&\prescript\otimes{}\vx^2&\cdots&\prescript\otimes{}\vx^n
\end{bmatrix}\in\R^{n\times n}
\)
and solve the linear equation
\[
  \prescript\otimes{}\mX^\top\begin{bmatrix}
    Q \\ P
  \end{bmatrix} 
  = \mathbf1 
\]
for \(Q\) and \(P\).\footnotemark{}
\footnotetext{
  Though \(Q\) is in \(\R^{n-d}\), we index it as \(Q_{i,j}\) for \(i=1,2,\ldots,d\) and \(i\leq j\leq d\) for notational convenience. One can check that this results in \(n-d\) indices.
}

This method for solving for a \(d\)-dimensional hyperellipsoid means that for a model with a hidden size of 512, or \(2^9\), 
we would need \(2^{17} + 2^9 + 2^8=\num{131 840}\) samples. 
More generally, the number of samples is \(O(d^2)\).
Typically for these attacks, a single prefix token is sent to the model to obtain a sample. 
However, as the number of samples surpasses the vocabulary size of the model it becomes necessary to send multi-token prefixes to the model in order to expand the number of unique prefixes.
The number of tokens per sample grows logarithmically with the number of samples required~\(n\), i.e., it grows at a rate of \(O(\log_vn)\). 
Combining this with the sample complexity as a function of model embedding size~\(d\) and the fact that we need \(d\) queries per sample means that 
the cost of executing the attack grows at a rate of~\(O(d^3\log_vd)\).

Since the cost grows super-cubically with the hidden size, 
current API pricing makes it prohibitively expensive to obtain scaling factors for many popular LLMs, as shown in Table~\ref{tab:price}.
Though OpenAI's cheapest and smallest available generative model, \texttt{babbage-002}, is only about \$\num{1000} to attack, \texttt{gpt-3.5-turbo} costs over \$\num{150 000}.
However, if historic trends continue, these costs could drop significantly in the future.

\begin{table}
  \centering
  \small
  \begin{threeparttable}
  \input{tab/models.tex}
    \begin{tablenotes}
    \item[a] Confirmed size from \citet{Carlini2024StealingPO}.
    \item[b] Estimated size upper limit from \citet{Finlayson2024LogitsOA}.
    \end{tablenotes}
  \end{threeparttable}
  \caption{
    A summary of models, their sizes, the number of samples required to ascertain their output ellipsoid, and the cost of obtaining the samples, based on OpenAI inference pricing on June 7, 2024. The number of samples required grows quadratically with the embedding size, and the price per sample grows logarithmically with the number of samples.
  }
  \label{tab:price}
\end{table}

\section{Model ellipses as weak signatures}

\lettrine{T}{he authors of \citet{Finlayson2024LogitsOA}} identify several useful applications for the model image. 
Among them is the idea of using the model's unique output space (its ``image'') 
as a type of \textit{signature} to uniquely identify model outputs.
One shortcoming of their proposal 
is that anyone can cheaply obtain and share the model image,
meaning that any model can be retrofitted to share the target LM's image. 
This, combined with the fact that the original model be also be easily modified to change its image, (e.g., by continued training), means that the image is not very useful as a signature in an adversarial setting.
The prohibitive cost of discovering the model ellipse, however, makes it more suitable for at least one particular application, which we will illustrate.

Suppose Alice has a proprietary LM, protected by an API.
As an act of revenge for his recent termination,
Alice's former employee Bob decides to use not-yet revoked access to the company servers to download the model.
Bob then goes to Candice and tries to sell his stolen model to her.
Candice, knowing Bob's unscrupulous history, but wanting cheap access to the LM,
wishes to verify that Bob's model is indeed the one he says it is.
Candice can do this by simply checking that every output from Alice's model resides on Bob's model's ellipse. 
This can even be done without requiring Bob to reveal his model's weights.
If Candice obtains a random output from Alice's model, she can reveal any \(d-3\) elements from this output to Bob. Once this is done, Bob can give Candice two candidate values for each remaining element in the output, one of which will be correct.

\begin{figure}
  \centering
  \small
  \begin{tikzpicture}[node distance=2cm]
    \node (A) {Alice};
    \node (B) [right=of A] {Bob};
    \draw[->] (A) -- node[above] {1. Steals LM} (B);
    \node (C) [right=of B] {Candice};
    \draw[->] (A) to[bend right] node [below] {2. Gets random \(\vell\)} (C);
    \draw[->] (C) to[bend right] node [above] {3. Reveals \(\vell_{:d-3}\)} (B);
    \draw[->] (B) to node [below] {4. Sends \(\vell_{d-2}\)} (C);
  \end{tikzpicture}
  \hfill
  \begin{tikzpicture}[
      steplabel/.style={draw, fill=white},
      transformlabel/.style={fill=white, rounded corners, inner sep=1pt}
    ]
    \newcommand\yradius{0.7}
    \coordinate (input) at (0,0);
    \draw[name path=ellipse] (input) circle [x radius=1.5, y radius=\yradius, rotate=30];
    \draw[<->, name path=line] (0.5,1.2) -- (0.5,-1) node [below] {1. Candice reveals \(\vell_0\)};
    \fill [name intersections={of=ellipse and line}] 
    (intersection-1) circle (1.5pt) 
    (intersection-2) circle (1.5pt) node [right] {\(\vell\)};
    \node[fill=white] (B) at (-1, -0.25) {2. Bob guesses \(\vell\)};
    \draw[->, shorten >=2pt] (B) -- (intersection-1);
    \draw[->, shorten >=2pt] (B) -- (intersection-2);
  \end{tikzpicture}
\end{figure}


\section{Ellipsoid fitting}

\lettrine{O}{btaining sufficient samples} from an API-protected LLM is only the first step in finding the model ellipse. 
The second step is to fit an ellipse to the samples.
This, it turns out, is also difficult. 
There has been progress over recent decades on fast algorithms for multi-dimensional ellipsoid fitting~\citep{Calafiore2002ApproximationON, Ying2012AFA}, however the best known method still requires \(O(d^6)\) time~\citep{Lin2016FastME} which is prohibitively polynomial, as illustrated in Figure~\ref{fig:eigh}.

\begin{figure}
  \centering
  \small
  \begin{tikzpicture}
    \begin{axis}[
        width=0.5\textwidth,
        height=4cm,
        xlabel={Dimension \(d\)},
        ylabel={Time (seconds)},
      ]
      \addplot[only marks] table {data/times.dat};
      \addplot[raw gnuplot] gnuplot {
        f(x) = a * x ** 6 + b * x ** 5 + c * x ** 4 + d * x ** 3 + e * x ** 2 + f * x + g;
        a = 1; b = 0; c = 0; d = 0; e = 0; f = 0; g = 0;
        fit f(x) 'data/times.dat' using 1:2 via a,b,c,d,e,f,g;
        plot[0:220] f(x);
      };
    \end{axis}
  \end{tikzpicture}
  \caption{
    Pictured here are the runtimes for taking the eigendecomposition of the symmetric matrix 
    composed of the quadratic terms of \(m\) \(d\)-dimensional points on an ellipsoid.
    Times are obtained on an M1 MacBook Pro
    using the \texttt{\_syevd} routine from LAPACK~\citep{laug} 
    via NumPy's \texttt{linalg.eigh} function~\citep{harris2020array}.
    This is the main computational bottleneck of the fastest known algorithm 
    for fitting a multi-dimensional elliposid, which takes \(O(d^6)\) time. 
    While still polynomial, this is a severe computational hurdle
    when LLMs have embedding dimensions in the thousands.
  }
  \label{fig:eigh}
\end{figure}

\bibliographystyle{chicago-annote}
\bibliography{refs}

\end{document}
